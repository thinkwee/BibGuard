"""
Report generator for bibliography check results.
"""
from dataclasses import dataclass
from datetime import datetime
from typing import Optional
from pathlib import Path

from ..parsers.bib_parser import BibEntry
from ..analyzers.metadata_comparator import ComparisonResult
from ..analyzers.usage_checker import UsageResult
from ..analyzers.llm_evaluator import EvaluationResult
from ..analyzers.duplicate_detector import DuplicateGroup


@dataclass
class EntryReport:
    """Complete report for a single bib entry."""
    entry: BibEntry
    comparison: Optional[ComparisonResult]
    usage: Optional[UsageResult]
    evaluations: list[EvaluationResult]


class ReportGenerator:
    """Generates formatted markdown reports."""
    
    def __init__(self):
        self.entries: list[EntryReport] = []
        self.missing_citations: list[str] = []
        self.duplicate_groups: list[DuplicateGroup] = []
        self.bib_file: str = ""
        self.tex_file: str = ""
    
    def add_entry_report(self, report: EntryReport):
        """Add an entry report."""
        self.entries.append(report)
    
    def set_metadata(self, bib_file: str, tex_file: str):
        """Set source file information."""
        self.bib_file = bib_file
        self.tex_file = tex_file
    
    def set_missing_citations(self, missing: list[str]):
        """Set list of citations without bib entries."""
        self.missing_citations = missing
    
    def set_duplicate_groups(self, groups: list[DuplicateGroup]):
        """Set list of duplicate entry groups."""
        self.duplicate_groups = groups
    
    def generate(self) -> str:
        """Generate the full markdown report."""
        lines = []
        
        # Header
        lines.extend(self._generate_header())
        lines.append("")
        
        # Disclaimer
        lines.extend(self._generate_disclaimer())
        lines.append("")
        
        # Summary statistics
        lines.extend(self._generate_summary())
        lines.append("")
        
        # âš ï¸ Critical Issues (Detailed) - PRIORITIZED
        lines.extend(self._generate_issues_section())
        lines.append("")
        
        # âœ… Verified Entries (Clean)
        lines.extend(self._generate_verified_section())
        lines.append("")
        
        # Footer
        lines.extend(self._generate_footer())
        
        return "\n".join(lines)

    def generate_console_output(self) -> str:
        """Generate console-friendly output (Summary + Issues only)."""
        lines = []
        
        # Summary statistics
        lines.extend(self._generate_summary())
        lines.append("")
        
        # Critical Issues
        lines.extend(self._generate_issues_section())
        lines.append("")
        
        return "\n".join(lines)
    
    def _generate_header(self) -> list[str]:
        """Generate report header."""
        bib_name = Path(self.bib_file).name if self.bib_file else "N/A"
        tex_name = Path(self.tex_file).name if self.tex_file else "N/A"
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        return [
            "# Bibliography Validation Report",
            "",
            f"**Generated:** {timestamp}",
            "",
            "| File Type | Filename |",
            "|-----------|----------|",
            f"| **Bib File** | `{bib_name}` |",
            f"| **TeX File** | `{tex_name}` |"
        ]

    def _generate_disclaimer(self) -> list[str]:
        """Generate disclaimer section."""
        return [
            "> **âš ï¸ Disclaimer:** This report is generated by an automated tool. While BibGuard strives for accuracy, it may produce false positives or miss certain issues. **This tool cannot replace human review.** Please manually verify all reported issues before making changes to your bibliography."
        ]
    
    def _generate_summary(self) -> list[str]:
        """Generate summary statistics."""
        total = len(self.entries)
        
        # Check availability of results
        has_metadata = any(e.comparison is not None for e in self.entries)
        has_usage = any(e.usage is not None for e in self.entries)
        has_eval = any(len(e.evaluations) > 0 for e in self.entries)
        
        # Calculate Verified/Issues
        # Note: _is_verified depends on _has_issues. 
        # If a check wasn't run, it won't contribute to issues.
        verified = sum(1 for e in self.entries if self._is_verified(e))
        issues = sum(1 for e in self.entries if self._has_issues(e))
        
        # Usage stats
        if has_usage:
            used = sum(1 for e in self.entries if e.usage and e.usage.is_used)
            unused = total - used
            used_str = str(used)
            unused_str = str(unused)
            missing_str = str(len(self.missing_citations))
        else:
            used_str = "N/A"
            unused_str = "N/A"
            missing_str = "N/A"
            
        # Duplicate stats
        dup_str = str(len(self.duplicate_groups)) if self.duplicate_groups else "0"

        return [
            "## ğŸ“Š Summary",
            "",
            "| Metric | Count |",
            "|--------|-------|",
            f"| **Total Entries** | {total} |",
            f"| âœ… **Verified (Clean)** | {verified} |",
            f"| âš ï¸ **With Issues** | {issues} |",
            f"| ğŸ“ **Used in TeX** | {used_str} |",
            f"| ğŸ—‘ï¸ **Unused** | {unused_str} |",
            f"| ğŸ”„ **Duplicate Groups** | {dup_str} |",
            f"| âŒ **Missing Bib Entries** | {missing_str} |"
        ]
    
    def _is_verified(self, entry: EntryReport) -> bool:
        """Check if entry is clean (no issues)."""
        return not self._has_issues(entry)

    def _has_issues(self, entry: EntryReport) -> bool:
        """Check if entry has any issues."""
        # Metadata issues
        if entry.comparison and entry.comparison.has_issues:
            return True
        # Usage issues (unused)
        if entry.usage and not entry.usage.is_used:
            return True
        # LLM issues (low relevance)
        if any(ev.relevance_score <= 2 for ev in entry.evaluations):
            return True
        return False

    def _generate_issues_section(self) -> list[str]:
        """Generate detailed section for entries with issues."""
        lines = ["## âš ï¸ Critical Issues Detected", ""]
        
        has_any_issues = False
        
        # 1. Missing Citations
        if self.missing_citations:
            has_any_issues = True
            lines.append("### âŒ Missing Bibliography Entries")
            lines.append("The following keys are cited in the TeX file but missing from the .bib file:")
            lines.append("")
            for key in self.missing_citations:
                lines.append(f"- `{key}`")
            lines.append("")

        # 2. Duplicate Entries
        if self.duplicate_groups:
            has_any_issues = True
            lines.append("### ğŸ”„ Duplicate Entries")
            for i, group in enumerate(self.duplicate_groups, 1):
                lines.append(f"#### Group {i} (Similarity: {group.similarity_score:.0%})")
                lines.append(f"**Reason:** {group.reason}")
                lines.append("")
                lines.append("| Key | Title | Year |")
                lines.append("|-----|-------|------|")
                for entry in group.entries:
                    lines.append(f"| `{entry.key}` | {entry.title} | {entry.year} |")
                lines.append("")

        # 3. Unused Entries
        unused = [e for e in self.entries if e.usage and not e.usage.is_used]
        if unused:
            has_any_issues = True
            lines.append("### ğŸ—‘ï¸ Unused Entries")
            lines.append("The following entries are in the .bib file but NOT cited in the TeX file:")
            lines.append("")
            for e in unused:
                lines.append(f"- `{e.entry.key}`: *{e.entry.title}*")
            lines.append("")

        # 4. Metadata Mismatches & Low Relevance
        issue_entries = [e for e in self.entries if self._has_issues(e)]
        
        if issue_entries:
            has_any_issues = True
            lines.append("### âš ï¸ Metadata & Relevance Issues")
            
            for entry_report in issue_entries:
                lines.extend(self._format_entry_detail(entry_report))

        if not has_any_issues:
            lines.append("ğŸ‰ **No critical issues found!**")

        return lines

    def _generate_verified_section(self) -> list[str]:
        """Generate section for verified entries."""
        lines = ["## âœ… Verified Entries", ""]
        
        verified = [e for e in self.entries if self._is_verified(e)]
        
        if not verified:
            lines.append("_No verified entries found._")
            return lines
            
        lines.append(f"Found **{len(verified)}** entries with correct metadata.")
        lines.append("")
        
        # Use a collapsible details block for clean UI
        lines.append("<details>")
        lines.append("<summary>Click to view verified entries</summary>")
        lines.append("")
        
        for entry_report in verified:
            lines.extend(self._format_entry_detail(entry_report, minimal=True))
            
        lines.append("</details>")
        return lines

    def _format_entry_detail(self, report: EntryReport, minimal: bool = False) -> list[str]:
        """Format a single entry report in Markdown."""
        entry = report.entry
        comp = report.comparison
        lines = []
        
        # Title header
        icon = "âœ…" if minimal else "âš ï¸"
        lines.append(f"#### {icon} `{entry.key}`")
        lines.append(f"**Title:** {entry.title}")
        lines.append("")
        
        # Metadata Status
        if comp:
            status_icon = "âœ…" if comp.is_match else "âŒ"
            lines.append(f"- **Metadata Status:** {status_icon} {comp.source.upper()} (Confidence: {comp.confidence:.1%})")
            
            if comp.has_issues and not minimal:
                lines.append("  - **Discrepancies:**")
                for issue in comp.issues:
                     # Format mismatch details nicely
                    if "Mismatch" in issue or "mismatch" in issue:
                        lines.append(f"    - ğŸ”´ {issue}")
                        if "Title" in issue:
                            lines.append(f"      - **Bib:** `{comp.bib_title}`")
                            lines.append(f"      - **Fetched:** `{comp.fetched_title}`")
                        elif "Author" in issue:
                            lines.append(f"      - **Bib:** `{', '.join(comp.bib_authors)}`")
                            lines.append(f"      - **Fetched:** `{', '.join(comp.fetched_authors)}`")
                    else:
                        lines.append(f"    - ğŸ”¸ {issue}")
        
        # Relevance Status
        if report.evaluations and not minimal:
            lines.append("- **Relevance Analysis:**")
            for eval_res in report.evaluations:
                score_icon = "ğŸŸ¢" if eval_res.relevance_score >= 4 else ("ğŸŸ¡" if eval_res.relevance_score == 3 else "ğŸ”´")
                lines.append(f"  - {score_icon} **Score {eval_res.relevance_score}/5** ({eval_res.score_label})")
                if eval_res.line_number:
                    lines.append(f"    - Line {eval_res.line_number}")
                lines.append(f"    - *\"{eval_res.explanation}\"*")

        lines.append("")
        lines.append("---")
        lines.append("")
        return lines

    def _generate_footer(self) -> list[str]:
        """Generate report footer."""
        return [
            "",
            "---",
            f"Report generated by **BibGuard** on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
        ]
    
    def save(self, filepath: str):
        """Save report to file."""
        content = self.generate()
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)
