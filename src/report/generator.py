"""
Report generator for bibliography check results.
Outputs native Markdown format for better readability.
"""
from dataclasses import dataclass
from datetime import datetime
from typing import Optional, List
from pathlib import Path

from ..parsers.bib_parser import BibEntry
from ..analyzers.metadata_comparator import ComparisonResult
from ..analyzers.usage_checker import UsageResult
from ..analyzers.llm_evaluator import EvaluationResult
from ..analyzers.duplicate_detector import DuplicateGroup
from ..analyzers.field_completeness_checker import FieldCompletenessResult
from ..analyzers.url_validator import URLValidationResult
from ..analyzers.venue_normalizer import VenueNormalizationResult


@dataclass
class EntryReport:
    """Complete report for a single bib entry."""
    entry: BibEntry
    comparison: Optional[ComparisonResult]
    usage: Optional[UsageResult]
    evaluations: List[EvaluationResult]


class ReportGenerator:
    """Generates formatted Markdown reports."""
    
    def __init__(self):
        self.entries: List[EntryReport] = []
        self.missing_citations: List[str] = []
        self.duplicate_groups: List[DuplicateGroup] = []
        self.field_results: List[FieldCompletenessResult] = []
        self.url_results: List[URLValidationResult] = []
        self.venue_results: List[VenueNormalizationResult] = []
        self.bib_file: str = ""
        self.tex_file: str = ""
    
    def add_entry_report(self, report: EntryReport):
        """Add an entry report."""
        self.entries.append(report)
    
    def set_metadata(self, bib_file: str, tex_file: str):
        """Set source file information."""
        self.bib_file = bib_file
        self.tex_file = tex_file
    
    def set_missing_citations(self, missing: List[str]):
        """Set list of citations without bib entries."""
        self.missing_citations = missing
    
    def set_duplicate_groups(self, groups: List[DuplicateGroup]):
        """Set list of duplicate entry groups."""
        self.duplicate_groups = groups
    
    def set_field_completeness_results(self, results: List[FieldCompletenessResult]):
        """Set field completeness check results."""
        self.field_results = results
    
    def set_url_validation_results(self, results: List[URLValidationResult]):
        """Set URL validation results."""
        self.url_results = results
    
    def set_venue_normalization_results(self, results: List[VenueNormalizationResult]):
        """Set venue normalization results."""
        self.venue_results = results
    
    def generate(self) -> str:
        """Generate the full Markdown report."""
        sections = []
        
        # Header
        sections.append(self._generate_header())
        
        # Summary table
        sections.append(self._generate_summary())
        
        # Duplicates
        if self.duplicate_groups:
            sections.append(self._generate_duplicates_section())
        
        # Field completeness
        if self.field_results:
            sections.append(self._generate_field_completeness_section())
        
        # URL validation
        if self.url_results:
            sections.append(self._generate_url_validation_section())
        
        # Venue normalization
        if self.venue_results:
            sections.append(self._generate_venue_section())
        
        # Metadata validation
        sections.append(self._generate_metadata_section())
        
        # Usage analysis
        sections.append(self._generate_usage_section())
        
        # LLM evaluation
        sections.append(self._generate_evaluation_section())
        
        # Recommendations
        sections.append(self._generate_recommendations())
        
        return "\n\n".join(sections)
    
    def _generate_header(self) -> str:
        """Generate report header with disclaimer."""
        bib_name = Path(self.bib_file).name if self.bib_file else "N/A"
        tex_name = Path(self.tex_file).name if self.tex_file else "N/A"
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        return f"""# ğŸ“š BibGuard Validation Report

---
*Report generated by BibGuard*

> [!WARNING]
> **Please note the following limitations:**
> 1. Online paper searches may still have errors. Technical blogs and non-paper references cannot be verified.
> 2. Some papers may not have retrievable abstracts, making LLM relevance judgments unreliable.
> 3. Use a capable LLM model for relevance checks (doesn't require many tokens), but even the best models may produce errors or hallucinations.
> 4. This tool is only an aid for detection and cannot replace manual review.

| Property | Value |
|----------|-------|
| **Bib File** | `{bib_name}` |
| **TeX File** | `{tex_name}` |
| **Generated** | {timestamp} |"""
    
    def _generate_summary(self) -> str:
        """Generate summary statistics."""
        total = len(self.entries)
        verified = sum(1 for e in self.entries if e.comparison and e.comparison.is_match)
        issues = sum(1 for e in self.entries if e.comparison and e.comparison.has_issues)
        used = sum(1 for e in self.entries if e.usage and e.usage.is_used)
        unused = total - used
        evaluated_entries = sum(1 for e in self.entries if e.evaluations)
        total_evaluations = sum(len(e.evaluations) for e in self.entries)
        relevant_citations = sum(
            sum(1 for eval_res in e.evaluations if eval_res.is_relevant)
            for e in self.entries
        )
        
        return f"""## ğŸ“Š Summary

| Metric | Count |
|--------|------:|
| Total Entries | {total} |
| âœ… Verified (match) | {verified} |
| âš ï¸ With Issues | {issues} |
| ğŸ“– Used in TeX | {used} |
| ğŸš« Unused | {unused} |
| ğŸ‘¯ Duplicate Groups | {len(self.duplicate_groups)} |
| ğŸ¤– Entries Evaluated | {evaluated_entries} |
| ğŸ“ Total Citations Checked | {total_evaluations} |
| âœ“ Relevant Citations | {relevant_citations} |
| âŒ Missing Bib Entries | {len(self.missing_citations)} |"""
    
    def _generate_duplicates_section(self) -> str:
        """Generate duplicate detection section."""
        if not self.duplicate_groups:
            return "## ğŸ‘¯ Duplicate Detection\n\nâœ… No duplicate entries detected."
        
        lines = [f"## ğŸ‘¯ Duplicate Detection\n\nâš ï¸ Found **{len(self.duplicate_groups)}** potential duplicate groups:\n"]
        
        for i, group in enumerate(self.duplicate_groups, 1):
            lines.append(f"### Group {i} ({group.similarity_score:.0%} similar)")
            lines.append(f"\n**Reason:** {group.reason}\n")
            lines.append("| Entry Key | Title | Year |")
            lines.append("|-----------|-------|------|")
            for entry in group.entries:
                title = self._clean_text(entry.title)
                lines.append(f"| `{entry.key}` | {title} | {entry.year or 'N/A'} |")
            lines.append("")
        
        return "\n".join(lines)
    
    def _generate_field_completeness_section(self) -> str:
        """Generate field completeness check section."""
        if not self.field_results:
            return "## ğŸ“‹ Field Completeness\n\nâœ… All entries have required fields."
        
        lines = [f"## ğŸ“‹ Field Completeness\n\nâš ï¸ Found **{len(self.field_results)}** entries with missing fields:\n"]
        lines.append("| Entry Key | Type | Missing Required | Missing Recommended |")
        lines.append("|-----------|------|------------------|---------------------|")
        
        for result in self.field_results:
            req = ", ".join(result.missing_required) if result.missing_required else "â€”"
            rec = ", ".join(result.missing_recommended) if result.missing_recommended else "â€”"
            lines.append(f"| `{result.entry_key}` | {result.entry_type} | {req} | {rec} |")
        
        return "\n".join(lines)
    
    def _generate_url_validation_section(self) -> str:
        """Generate URL validation section."""
        if not self.url_results:
            return "## ğŸ”— URL/DOI Validation\n\nâœ… All URLs and DOIs are valid."
        
        lines = [f"## ğŸ”— URL/DOI Validation\n\nâš ï¸ Found **{len(self.url_results)}** invalid URLs/DOIs:\n"]
        lines.append("| Entry Key | Type | URL | Error |")
        lines.append("|-----------|------|-----|-------|")
        
        for result in self.url_results:
            url = self._clean_text(result.url)
            lines.append(f"| `{result.entry_key}` | {result.url_type.upper()} | `{url}` | {result.error} |")
        
        return "\n".join(lines)
    
    def _generate_venue_section(self) -> str:
        """Generate venue normalization section."""
        if not self.venue_results:
            return "## ğŸ›ï¸ Venue Consistency\n\nâœ… Venue names are consistent."
        
        lines = [f"## ğŸ›ï¸ Venue Consistency\n\nâš ï¸ Found **{len(self.venue_results)}** venue naming inconsistencies:\n"]
        
        for result in self.venue_results:
            lines.append(f"### {result.canonical_name}")
            lines.append(f"\n**Suggested:** `{result.suggested_name}`\n")
            lines.append("| Variant | Count | Entries |")
            lines.append("|---------|------:|---------|")
            for variant in result.variants:
                entries = ", ".join(f"`{k}`" for k in variant.entry_keys[:3])
                if len(variant.entry_keys) > 3:
                    entries += f" +{len(variant.entry_keys) - 3} more"
                lines.append(f"| {variant.name} | {variant.count} | {entries} |")
            lines.append("")
        
        return "\n".join(lines)
    
    def _generate_metadata_section(self) -> str:
        """Generate metadata validation section."""
        lines = ["## ğŸ” Metadata Validation\n"]
        
        # Group by status
        verified = []
        issues = []
        unverified = []
        
        for entry_report in self.entries:
            entry = entry_report.entry
            comp = entry_report.comparison
            
            if comp:
                if comp.is_match:
                    verified.append((entry, comp))
                elif comp.has_issues:
                    issues.append((entry, comp))
                else:
                    unverified.append((entry, comp))
            else:
                unverified.append((entry, None))
        
        # Show entries with issues first (expanded)
        if issues:
            lines.append(f"### âš ï¸ Entries with Issues ({len(issues)})\n")
            for entry, comp in issues:
                lines.append(self._format_entry_details(entry, comp))
        
        # Verified entries (collapsed)
        if verified:
            lines.append(f"\n<details>\n<summary>âœ… Verified Entries ({len(verified)})</summary>\n")
            lines.append("| Entry Key | Title | Source | Confidence |")
            lines.append("|-----------|-------|--------|------------|")
            for entry, comp in verified:
                title = self._clean_text(entry.title)
                lines.append(f"| `{entry.key}` | {title} | {comp.source} | {comp.confidence:.0%} |")
            lines.append("\n</details>")
        
        # Unverified entries (collapsed)
        if unverified:
            lines.append(f"\n<details>\n<summary>â“ Could Not Verify ({len(unverified)})</summary>\n")
            for entry, comp in unverified:
                lines.append(f"- `{entry.key}`: {self._clean_text(entry.title)}")
            lines.append("\n</details>")
        
        return "\n".join(lines)
    
    def _format_entry_details(self, entry: BibEntry, comp: ComparisonResult) -> str:
        """Format detailed entry comparison."""
        lines = [f"#### `{entry.key}`\n"]
        lines.append(f"- **Type:** {entry.entry_type}")
        lines.append(f"- **Source:** {comp.source.upper()}")
        lines.append(f"- **Confidence:** {comp.confidence:.0%}")
        
        if comp.issues:
            lines.append("\n**Issues:**")
            for issue in comp.issues:
                lines.append(f"- {issue}")
        
        # Show discrepancies
        if comp.title_similarity < 0.999:
            lines.append(f"\n**Title Mismatch** ({comp.title_similarity:.0%} similar)")
            lines.append(f"- Bib: _{self._clean_text(comp.bib_title)}_")
            lines.append(f"- Fetched: _{self._clean_text(comp.fetched_title)}_")
        
        if comp.author_similarity < 0.999:
            lines.append(f"\n**Author Mismatch** ({comp.author_similarity:.0%} similar)")
            lines.append(f"- Bib: {', '.join(comp.bib_authors[:3])}")
            lines.append(f"- Fetched: {', '.join(comp.fetched_authors[:3])}")
        
        if not comp.year_match:
            lines.append(f"\n**Year Mismatch:** Bib=`{comp.bib_year}`, Fetched=`{comp.fetched_year}`")
        
        lines.append("")
        return "\n".join(lines)
    
    def _generate_usage_section(self) -> str:
        """Generate usage analysis section."""
        lines = ["## ğŸ“– Citation Usage\n"]
        
        # Unused entries
        unused = [e for e in self.entries if e.usage and not e.usage.is_used]
        if unused:
            lines.append(f"### ğŸš« Unused Entries ({len(unused)})\n")
            lines.append("These entries are in the `.bib` file but not cited in the document:\n")
            for entry_report in unused:
                lines.append(f"- `{entry_report.entry.key}`")
            lines.append("")
        
        # Missing citations
        if self.missing_citations:
            lines.append(f"### âŒ Missing Entries ({len(self.missing_citations)})\n")
            lines.append("These are cited in the document but not in the `.bib` file:\n")
            for key in self.missing_citations:
                lines.append(f"- `{key}`")
            lines.append("")
        
        # Usage frequency
        used_entries = sorted(
            [(e.entry.key, e.usage.usage_count) for e in self.entries if e.usage and e.usage.is_used],
            key=lambda x: x[1],
            reverse=True
        )
        
        if used_entries:
            lines.append("<details>\n<summary>ğŸ“Š Citation Frequency</summary>\n")
            lines.append("| Entry Key | Citations |")
            lines.append("|-----------|----------:|")
            for key, count in used_entries[:20]:
                bar = "â–ˆ" * min(count, 10)
                lines.append(f"| `{key}` | {count} {bar} |")
            if len(used_entries) > 20:
                lines.append(f"\n*...and {len(used_entries) - 20} more*")
            lines.append("\n</details>")
        
        if not unused and not self.missing_citations:
            lines.append("âœ… All bib entries are cited in the document.")
        
        return "\n".join(lines)
    
    def _generate_evaluation_section(self) -> str:
        """Generate LLM evaluation section."""
        lines = ["## ğŸ¤– Citation Relevance Analysis\n"]
        
        evaluated = [e for e in self.entries if e.evaluations]
        
        if not evaluated:
            return "## ğŸ¤– Citation Relevance Analysis\n\n*No LLM evaluation performed.*"
        
        # Collect all evaluations
        all_evaluations = []
        for entry_report in evaluated:
            for eval_res in entry_report.evaluations:
                all_evaluations.append((entry_report.entry, eval_res))
        
        # Group by score
        score_labels = {
            5: "ğŸŒŸ Highly Relevant",
            4: "âœ… Relevant", 
            3: "ğŸ”¸ Somewhat Relevant",
            2: "âš ï¸ Marginally Relevant",
            1: "âŒ Not Relevant",
            0: "â“ Unknown/Error"
        }
        
        evals_by_score = {5: [], 4: [], 3: [], 2: [], 1: [], 0: []}
        for entry, eval_res in all_evaluations:
            score = eval_res.relevance_score
            if score not in evals_by_score:
                score = 0
            evals_by_score[score].append((entry, eval_res))
        
        # Summary table
        lines.append("| Score | Count |")
        lines.append("|-------|------:|")
        for score in range(5, -1, -1):
            count = len(evals_by_score[score])
            if count > 0:
                lines.append(f"| {score_labels[score]} | {count} |")
        lines.append("")
        
        # Details for low relevance (collapsed for others)
        for score in range(5, -1, -1):
            group_evals = evals_by_score[score]
            if not group_evals:
                continue
            
            # Low relevance shown expanded, others collapsed
            if score <= 2:
                lines.append(f"### {score_labels[score]} ({len(group_evals)} citations)\n")
                for entry, eval_res in group_evals:
                    lines.append(self._format_evaluation(entry, eval_res))
            else:
                lines.append(f"\n<details>\n<summary>{score_labels[score]} ({len(group_evals)} citations)</summary>\n")
                for entry, eval_res in group_evals:
                    lines.append(self._format_evaluation(entry, eval_res))
                lines.append("\n</details>")
        
        return "\n".join(lines)
    
    def _format_evaluation(self, entry: BibEntry, eval_res: EvaluationResult) -> str:
        """Format a single evaluation result."""
        lines = [f"#### `{entry.key}`"]
        if eval_res.line_number:
            lines[0] += f" (line {eval_res.line_number})"
        lines.append("")
        
        # Context (full)
        context = self._clean_text(eval_res.context_used)
        lines.append(f"> {context}")
        lines.append("")
        lines.append(f"**Explanation:** {eval_res.explanation}")
        lines.append("")
        
        return "\n".join(lines)
    
    def _generate_recommendations(self) -> str:
        """Generate recommendations section."""
        lines = ["## ğŸ’¡ Recommendations\n"]
        
        recommendations = []
        
        # Unused entries
        unused = [e for e in self.entries if e.usage and not e.usage.is_used]
        if unused:
            recommendations.append(f"- Remove **{len(unused)}** unused bibliography entries or add citations for them.")
        
        # Missing entries
        if self.missing_citations:
            recommendations.append(f"- Add **{len(self.missing_citations)}** missing bibliography entries:")
            for key in self.missing_citations[:10]:
                recommendations.append(f"  - `{key}`")
            if len(self.missing_citations) > 10:
                recommendations.append(f"  - *...and {len(self.missing_citations) - 10} more*")
        
        # Metadata issues
        issues = [e for e in self.entries if e.comparison and e.comparison.has_issues]
        if issues:
            recommendations.append(f"- Review **{len(issues)}** entries with metadata discrepancies.")
        
        # Low relevance citations
        low_relevance = []
        for e in self.entries:
            for ev in e.evaluations:
                if ev.relevance_score <= 2:
                    low_relevance.append((e.entry.key, ev))
        
        if low_relevance:
            recommendations.append(f"- Review **{len(low_relevance)}** citations with low relevance scores (â‰¤2/5):")
            for key, ev in low_relevance[:5]:
                line_info = f" (line {ev.line_number})" if ev.line_number else ""
                recommendations.append(f"  - `{key}`{line_info}: Score {ev.relevance_score}/5")
            if len(low_relevance) > 5:
                recommendations.append(f"  - *...and {len(low_relevance) - 5} more*")
        
        # Field completeness
        if self.field_results:
            recommendations.append(f"- Add missing fields to **{len(self.field_results)}** entries.")
        
        # URL issues
        if self.url_results:
            recommendations.append(f"- Fix **{len(self.url_results)}** invalid URLs/DOIs.")
        
        # Venue consistency
        if self.venue_results:
            recommendations.append(f"- Normalize **{len(self.venue_results)}** venue names for consistency.")
        
        if not recommendations:
            lines.append("âœ… No major issues found. Your bibliography looks good!")
        else:
            lines.extend(recommendations)
        
        lines.append("\n---\n*Report generated by BibGuard*")
        
        return "\n".join(lines)
    
    def _clean_text(self, text: str) -> str:
        """Clean text for Markdown output (escape special chars)."""
        if not text:
            return ""
        # Replace newlines and escape pipe characters for Markdown tables
        return text.replace("\n", " ").replace("|", "\\|")
    
    def save(self, filepath: str):
        """Save report to file."""
        content = self.generate()
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)
